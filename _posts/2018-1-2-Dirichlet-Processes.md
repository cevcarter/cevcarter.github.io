---
title: Clustering with Dirichlet Processes
layout: post
---
# Unsupervised Clustering using Dirichlet Processes

Commonly, some data mining tasks will require the researcher to identify clusters in a dataset. Often this is done using a supervised algorithm, where the researcher specifies the number of clusters in the dataset prior to performing the analysis. A common example is the k-means algorithm, which is a simple non-probabilistic algorithm that seeks to minimize intra-cluster variance. 

However, it is also useful to have an algorithm that can determine the number of clusters in a dataset during the analysis. To do this requires techniques from nonparametric statistics, where the number of parameters in the model is allowed to increase with the size of the dataset. For clustering, this nonparametric solution typically uses a model known as a Dirichlet Process. 

I will first introduce a simple example of a Dirichlet Process, and then implement the MCMC solution for the mean-likelihood and Bayesian treatments of the posterior.


```python
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
```

The dataset we will use for a simple example is the famous Old Faithful dataset, with two prominant clusters.


```python
faithful = pd.read_csv("faithful.csv")
```


```python
plt.scatter(faithful["duration"][:50], faithful["waiting"][:50], s=10)
plt.show()
```


![png]({{"images/Dirichlet-Processes/output_5_0.png"|absolute_url}})


A Dirichlet process is parameterized by a base distribution $H_0$ and a scalar $\alpha$, and is denoted $DP(H_0, \alpha)$. These parameters describe a probability distribution over probability distributions; samples from a Dirichlet Process are themselves probability distributions. 

The process generates an infinite number of masses $\beta_i$ such that $\sum_i \beta_i = 1$ using a method  known as "stick breaking", dependent on $\alpha$. It also generates a set of locations $x_i$ which are drawn iid from $H_0$. 

For our purposes, these locations and masses constitute a set of centers and weights (respectively) for a mixture model with an infinite number of components. Due to limits of modern computation and to the dismay of my pure mathematics friends, the total number of components generated by the mixture model will be truncated to a finite number which should be "good enough". Sorry. Statistics is fundamentally applied. 


```python
def stick_breaking(alpha, K):
    beta_samples = np.random.beta(1, alpha, K)

    beta = np.zeros_like(beta_samples)
    for i, bs in enumerate(beta_samples):
        beta[i] = bs * np.subtract(1, beta_samples[0:i]).prod()
    
    return beta
```

Below, we generate the masses and locations and use them in a simple mixture model. In this model, we also sample the widths of each component from a Gamma distribution.


```python
from scipy.stats import norm

K = 10
alpha = 3

beta = stick_breaking(alpha, K)
mu = np.random.normal(0, 2, K)

x = np.arange(-5, 5, .05)
tau = np.random.gamma(1, 2, K)

components = np.zeros((K, len(x)))
for k in range(K):
    components[k,:] = beta[k] * norm(mu[k], tau[k]**-1).pdf(x)
    
mix = components.sum(axis=0)

fig, ax = plt.subplots(2, 1, figsize=(10,5), sharex=True)

ax[0].stem(mu, beta)
ax[0].set_title("Weights and Locations")

ax[1].plot(x, mix, color="black")
for c in components:
    ax[1].plot(x, c, color="blue", alpha=.3)
ax[1].set_title("Mixture")
plt.show()
```


![png]({{"images/Dirichlet-Processes/output_9_0.png"|absolute_url}})


For simplicity, we will use spherical Gaussians as the mixture components for the multidimensional case, and attempt to fit them to the data. This approach will identify anything in the data that looks roughly like a spherical Gaussian data as a cluster. The following function defines the fitted pdf. 


```python
def pD(x, beta, mu, tau): # spherical multivariate Gaussian
    K = len(beta)
    p = 0
    for k in range(K):
        p += beta[k] * (tau[k] / (2 * np.pi))**.5 * np.exp(-.5 * tau[k] * np.sum(np.square(x - mu[k,:])))
    return p
```

### Mean Likelihood Solution

For simplicity, we will consider the mean likelihood solution, which regards each prior parameter as coming from a sufficiently wide uniform solution. Since I don't know Theano, I can't use pymc3 to sample from the posterior, so I have to do the MCMC myself. 


```python
N = 700 # samples
K = 10 # components
X = faithful.values # data
X = (X - X.mean(axis=0)) / X.std(axis=0)
X = X[:50,:]

D = X.shape[1] # dimensions

alpha_samples = []
mu_samples = []
tau_samples = []
beta_samples = []

alpha = 1
mu = np.zeros((K, D))
tau = np.ones(K)

step = .01

logp = np.log(1e-300)
n = 0
while n < N:
    
    # get samples
    alphac = np.random.normal(alpha, step)
    while alphac <= 0:
        alphac = np.random.normal(alpha, step)
        
    betac = stick_breaking(alphac, K)
    muc = np.zeros_like(mu)
    for k in range(K):
        muc[k,:] = np.random.multivariate_normal(mu[k,:], step * np.eye(D))
    
    tauc = np.random.multivariate_normal(tau, step * np.eye(K))
    while np.any(tauc <= 0):
        tauc = np.random.multivariate_normal(tau, step * np.eye(K))

    # get likelihood
    logpc = 0
    for i in range(X.shape[0]):
        x = X[i,:]
        logpc += np.log(pD(x, betac, muc, tauc))
                
    # accept or reject samples
    u = np.random.uniform(0, 1)

    if logpc - logp >= np.log(u):
        n += 1
        
        logp = logpc
        
        alpha = alphac
        mu = muc
        tau = tauc
        
        print(n, alpha, logp)
        
        alpha_samples.append(alphac)
        beta_samples.append(betac)
        mu_samples.append(muc)
        tau_samples.append(tauc)        
```

 



```python
idx_len = 200
skip_len = 10
sample_idx = np.arange(n)[-idx_len:][::skip_len]
sample_idx
```




    array([500, 510, 520, 530, 540, 550, 560, 570, 580, 590, 600, 610, 620,
           630, 640, 650, 660, 670, 680, 690])



We can visualize the mean of the posterior.


```python
from matplotlib.patches import Circle
from matplotlib.collections import PatchCollection

mu_est = np.array(mu_samples)[sample_idx].mean(axis=0)
tau_est = np.array(tau_samples)[sample_idx].mean(axis=0)
beta_est = np.array(beta_samples)[sample_idx].mean(axis=0)

fig, ax = plt.subplots(1, 3, figsize=(20,10))

ax[0].stem([i for i in range(1, K+1)], beta_est)
ax[0].set_title("Weights")

for k in range(K):
    ax[1].add_patch(Circle(mu_est[k,:], tau_est[k]**-1, alpha=.7 * beta_est[k]/max(beta_est), edgecolor="black"))  
ax[1].scatter(mu_est[:,0], mu_est[:,1], color="black", s=10)
ax[1].axis("equal")
ax[1].set_title("Locations and Widths")

ax[2].scatter(X[:,0], X[:,1], color="k", s=10)
for k in range(K):
    ax[2].add_patch(Circle(mu_est[k,:], tau_est[k]**-1, alpha=.7 * beta_est[k]/max(beta_est), edgecolor="black"))  
ax[2].axis("equal")
ax[2].set_title("Clusters and Data")

plt.show()
```


![png]({{"images/Dirichlet-Processes/output_17_0.png"|absolute_url}})


### Bayesian Solution

So far, we have not considered appropriate priors for each of the parameters. In the Bayesian approach, we will do just that. This will allow for more appropriate parameter estimates and will decrease the likelihood of overfitting the model. Below is the modified MCMC sampling code for the Bayesian treatment. 


```python
from scipy import stats

N = 700 # samples
K = 10 # components
X = faithful.values # data
X = (X - X.mean(axis=0)) / X.std(axis=0)
X = X[:50,:]

D = X.shape[1] # dimensions

alpha_samples = []
mu_samples = []
tau_samples = []
beta_samples = []

alpha = 1
mu = np.zeros((K, D))
tau = np.ones(K)

step = .01

logp = np.log(1e-300)
n = 0
while n < N:
    
    # get samples
    alphac = np.random.normal(alpha, step)
    while alphac <= 0:
        alphac = np.random.normal(alpha, step)
        
    betac = stick_breaking(alphac, K)
    muc = np.zeros_like(mu)
    for k in range(K):
        muc[k,:] = np.random.multivariate_normal(mu[k,:], step * np.eye(D))
    
    tauc = np.random.normal(tau, step)
    while np.any(tauc <= 0):
        tauc = np.random.normal(tau, step)

    # get likelihood
    logpc = 0
    for i in range(X.shape[0]):
        x = X[i,:]
        logpc += np.log(pD(x, betac, muc, tauc))
        
    # priors
    logp_alphac = stats.gamma.logpdf(alphac, 1, .5)
    logp_muc = 0
    for k in range(K):
        logp_muc += stats.multivariate_normal.logpdf(muc[k,:], np.zeros(D), 5 * np.eye(D))
    logp_tauc = stats.gamma.logpdf(tau, 1, .5).sum()
    
    logpc += logp_alphac + logp_muc + logp_tauc
                
    # accept or reject samples
    u = np.random.uniform(0, 1)

    if logpc - logp >= np.log(u):
        n += 1
        
        logp = logpc
        
        alpha = alphac
        mu = muc
        tau = tauc
        
        print(n, alpha, logp)
        
        alpha_samples.append(alphac)
        beta_samples.append(betac)
        mu_samples.append(muc)
        tau_samples.append(tauc)

```

```python
idx_len = 200
skip_len = 10
sample_idx = np.arange(n)[-idx_len:][::skip_len]
sample_idx
```




    array([500, 510, 520, 530, 540, 550, 560, 570, 580, 590, 600, 610, 620,
           630, 640, 650, 660, 670, 680, 690])



Again, we can visualize the mean of the posterior. 


```python
from matplotlib.patches import Circle
from matplotlib.collections import PatchCollection

mu_est = np.array(mu_samples)[sample_idx].mean(axis=0)
tau_est = np.array(tau_samples)[sample_idx].mean(axis=0)
beta_est = np.array(beta_samples)[sample_idx].mean(axis=0)

fig, ax = plt.subplots(1, 3, figsize=(20,10))

ax[0].stem([i for i in range(1, K+1)], beta_est)
ax[0].set_title("Weights")

for k in range(K):
    ax[1].add_patch(Circle(mu_est[k,:], tau_est[k]**-1, alpha=.7 * beta_est[k]/max(beta_est), edgecolor="black"))  
ax[1].scatter(mu_est[:,0], mu_est[:,1], color="black", s=10)
ax[1].axis("equal")
ax[1].set_title("Locations and Widths")

ax[2].scatter(X[:,0], X[:,1], color="k", s=10)
for k in range(K):
    ax[2].add_patch(Circle(mu_est[k,:], tau_est[k]**-1, alpha=.7 * beta_est[k]/max(beta_est), edgecolor="black"))  
ax[2].axis("equal")
ax[2].set_title("Clusters and Data")

plt.show()
```


![png]({{"images/Dirichlet-Processes/output_23_0.png"|absolute_url}})


Clearly, the Bayesian approach gives similar results to the mean likelihood. Experimentally, the Bayesian treatment also gives more consistent results than the Mean Likelihood treatment as well. 

In any either case, this method of clustering may provide a basis for determining the number of clusters for a supervised clustering algorithm. This may be helpful for datasets that are difficult to inspect visually, such as collections of observations in high dimensional space. 

In this case, both methods have identified two clusters in the dataset. Using a vanilla k-means algorithm can verify this for us. 


```python
from sklearn.cluster import KMeans

km = KMeans(2)
km.fit(X)
class_pred = km.predict(X)

plt.scatter(X[:,0], X[:,1], c=class_pred, alpha=.7)
plt.show()
```


![png]({{"images/Dirichlet-Processes/output_25_0.png"|absolute_url}})

